[package]
name = "llm-pipeline"
version = "0.1.0"
edition = "2021"
authors = ["Josh"]
license = "MIT"
description = "Multi-stage LLM workflow orchestrator with streaming, thinking mode, and cancellation support"
keywords = ["llm", "ai", "pipeline", "workflow", "ollama"]
categories = ["asynchronous", "api-bindings"]

[dependencies]
tokio = { version = "1", features = ["full"] }
reqwest = { version = "0.12", features = ["json", "stream"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
anyhow = "1"
thiserror = "2"
futures = "0.3"

[dev-dependencies]
tokio-test = "0.4"

[[example]]
name = "basic_pipeline"
path = "examples/basic_pipeline.rs"

[[example]]
name = "streaming_pipeline"
path = "examples/streaming_pipeline.rs"

[[example]]
name = "thinking_mode"
path = "examples/thinking_mode.rs"

[[example]]
name = "context_injection"
path = "examples/context_injection.rs"
